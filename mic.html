<!DOCTYPE html>
<html>
<head>
  <title>Microphone Audio Level Display with Logging</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      background-color: #222;
      color: #fff;
      text-align: center;
      padding-top: 50px;
    }
    #volumeMeter {
      width: 80%;
      height: 20px;
      background-color: #555;
      margin: 20px auto;
      position: relative;
    }
    #volumeFill {
      height: 100%;
      background-color: lime;
      width: 0%;
      transition: width 0.1s;
    }
    #thresholdMarker {
      position: absolute;
      top: -5px;
      width: 2px;
      height: 30px;
      background-color: red;
      left: 50%;
    }
    #controls {
      margin-top: 30px;
    }
    input[type="range"] {
      width: 80%;
    }
    #logContainer {
      width: 80%;
      margin: 20px auto;
      text-align: left;
      background-color: #333;
      padding: 10px;
      max-height: 200px;
      overflow-y: auto;
      border: 1px solid #444;
    }
    #logContainer h2 {
      margin-top: 0;
    }
    .log-entry {
      font-size: 14px;
      margin-bottom: 5px;
    }
    #startButton {
      padding: 10px 20px;
      font-size: 18px;
      margin-top: 20px;
    }
  </style>
</head>
<body>

  <h1>Microphone Audio Level Display with Logging</h1>

  <div id="volumeMeter">
    <div id="volumeFill"></div>
    <div id="thresholdMarker"></div>
  </div>

  <div id="controls">
    <label for="threshold">Volume Threshold: <span id="thresholdValue">50</span></label><br>
    <input type="range" id="threshold" min="0" max="100" value="50">
  </div>

  <button id="startButton">Start Audio</button>

  <div id="logContainer">
    <h2>Logs</h2>
    <div id="logContent"></div>
  </div>

  <script>
    let audioContext;
    let analyser;
    let dataArray;
    let threshold = 50;
    let isAudioProcessing = false;

    const volumeMeter = document.getElementById('volumeMeter');
    const volumeFill = document.getElementById('volumeFill');
    const thresholdMarker = document.getElementById('thresholdMarker');
    const thresholdInput = document.getElementById('threshold');
    const thresholdValueDisplay = document.getElementById('thresholdValue');
    const startButton = document.getElementById('startButton');

    const logContainer = document.getElementById('logContent');

    function log(message) {
      console.log(message); // Also log to the console
      const logEntry = document.createElement('div');
      logEntry.className = 'log-entry';
      logEntry.textContent = message;
      logContainer.appendChild(logEntry);
      // Keep the latest log visible
      logContainer.scrollTop = logContainer.scrollHeight;
    }

    thresholdInput.addEventListener('input', () => {
      threshold = parseInt(thresholdInput.value);
      thresholdValueDisplay.textContent = threshold;
      updateThresholdMarker();
      log(`Threshold updated to ${threshold}`);
    });

    function updateThresholdMarker() {
      const meterWidth = volumeMeter.offsetWidth;
      thresholdMarker.style.left = (threshold / 100) * meterWidth - 1 + 'px';
    }

    async function setupAudio() {
      log('Setting up audio...');
      try {
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
        log(`AudioContext created. State: ${audioContext.state}`);

        // On iOS and some browsers, AudioContext may start in 'suspended' state
        if (audioContext.state === 'suspended') {
          await audioContext.resume();
          log(`AudioContext resumed. State: ${audioContext.state}`);
        }

        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        log('Microphone access granted.');

        const source = audioContext.createMediaStreamSource(stream);

        // Optional: Create a gain node to adjust volume
        const gainNode = audioContext.createGain();
        gainNode.gain.value = 1; // Increase if needed

        source.connect(gainNode);

        analyser = audioContext.createAnalyser();
        analyser.fftSize = 2048;
        const bufferLength = analyser.fftSize;
        dataArray = new Float32Array(bufferLength);

        gainNode.connect(analyser);

        log('Audio nodes connected.');

        isAudioProcessing = true;
        processAudio();
      } catch (err) {
        log(`Error accessing microphone: ${err.message}`);
        alert('Could not access microphone: ' + err.message);
      }
    }

    function processAudio() {
      if (!isAudioProcessing) {
        log('Audio processing stopped.');
        return;
      }

      requestAnimationFrame(processAudio);
      analyser.getFloatTimeDomainData(dataArray);

      let sumSquares = 0.0;
      for (let i = 0; i < dataArray.length; i++) {
        sumSquares += dataArray[i] * dataArray[i];
      }
      let rms = Math.sqrt(sumSquares / dataArray.length);
      let volumePercent = rms * 1000; // Multiply by 1000 for better scaling

      // Clamp volumePercent between 0 and 100
      volumePercent = Math.min(Math.max(volumePercent, 0), 100);

      volumeFill.style.width = volumePercent + '%';

      if (volumePercent > threshold) {
        volumeFill.style.backgroundColor = 'red';
      } else {
        volumeFill.style.backgroundColor = 'lime';
      }

      log(`Volume: ${volumePercent.toFixed(2)}%`);
    }

    startButton.addEventListener('click', () => {
      setupAudio();
      startButton.disabled = true;
    });

    // Stop audio processing when the page is unloaded
    window.addEventListener('beforeunload', () => {
      isAudioProcessing = false;
      if (audioContext && audioContext.state !== 'closed') {
        audioContext.close();
        log('AudioContext closed.');
      }
    });

    updateThresholdMarker();
  </script>

</body>
</html>